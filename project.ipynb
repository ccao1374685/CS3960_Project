{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: moviepy in c:\\users\\ccao2\\anaconda3\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in c:\\users\\ccao2\\anaconda3\\lib\\site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in c:\\users\\ccao2\\anaconda3\\lib\\site-packages (from moviepy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in c:\\users\\ccao2\\anaconda3\\lib\\site-packages (from moviepy) (2.32.3)\n",
      "Requirement already satisfied: proglog<=1.0.0 in c:\\users\\ccao2\\anaconda3\\lib\\site-packages (from moviepy) (0.1.11)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\ccao2\\anaconda3\\lib\\site-packages (from moviepy) (1.26.4)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in c:\\users\\ccao2\\anaconda3\\lib\\site-packages (from moviepy) (2.37.0)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in c:\\users\\ccao2\\anaconda3\\lib\\site-packages (from moviepy) (0.6.0)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\ccao2\\anaconda3\\lib\\site-packages (from imageio<3.0,>=2.5->moviepy) (10.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ccao2\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ccao2\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ccao2\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ccao2\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\ccao2\\anaconda3\\lib\\site-packages (from tqdm<5.0,>=4.11.2->moviepy) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install moviepy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from moviepy.editor import *\n",
    "import cv2\n",
    "from fer import FER\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from PIL import Image\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "\n",
    "# handle file selection, only allow mp4\n",
    "def get_file_path():\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select Video File\",\n",
    "        filetypes=[(\"MP4 Files\", \"*.mp4\")]  \n",
    "    )\n",
    "    return file_path\n",
    "\n",
    "# set up a Tkinter root to later display the top k frames \n",
    "root = tk.Tk()\n",
    "root.withdraw() \n",
    "\n",
    "#  get the user's video path \n",
    "video_path = get_file_path()\n",
    "\n",
    "segment_length = 10  # break clips into 10 second segments for easier analyzing \n",
    "os.makedirs(\"clips\", exist_ok=True)\n",
    "\n",
    "# load video path to video \n",
    "video = VideoFileClip(video_path)\n",
    "duration = int(video.duration) # get the duration of the video \n",
    "\n",
    "# use emtoion detector to detect emotion in actors' faces (use mtcnn = False to decrease runing time)\n",
    "emotion_detector = FER(mtcnn=False)\n",
    "\n",
    "# process the first clip \n",
    "i = 0\n",
    "clip = video.subclip(i, min(i + segment_length, duration))\n",
    "clip_path = f\"clips/clip_{i // segment_length}.mp4\"\n",
    "\n",
    "# write the clip to file \n",
    "clip.write_videofile(clip_path, codec=\"libx264\", audio_codec=\"aac\", verbose=False, logger=None)\n",
    "\n",
    "\n",
    "\n",
    "# open video with opencv\n",
    "clip_video = cv2.VideoCapture(clip_path)\n",
    "frame_count = 0\n",
    "emotion_data = []\n",
    "\n",
    "while True:\n",
    "    success, frame = clip_video.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # skip frames to decrease running time and also decrease frame similarity \n",
    "    if frame_count % 12 != 0:\n",
    "        frame_count += 1\n",
    "        continue\n",
    "    \n",
    "    # This portion was completed with the assitance with ChatGPT. Its purpose was to guide me on \n",
    "    # the FER library to detect emotions and collect metatdata in video frames. \n",
    "    # also used in conjunction with https://www.geeksforgeeks.org/facial-expression-recognizer-using-fer-using-deep-neural-net/ for FER details. \n",
    "    \n",
    "    # use emotion detector to find emotion in frames. \n",
    "    emotion = emotion_detector.top_emotion(frame)\n",
    "    if emotion and emotion[1] is not None:\n",
    "\n",
    "        # create emotion text to display \n",
    "        emotion_text = f\"{emotion[0]}: {emotion[1]:.2f}\"\n",
    "\n",
    "        # store to use to display later\n",
    "        emotion_data.append((frame, emotion_text, emotion[1], frame_count))\n",
    "\n",
    "    frame_count += 1\n",
    "    \n",
    "clip_video.release()\n",
    "\n",
    "\n",
    "#sort the data from highest to lowest \n",
    "emotion_sorted = sorted(emotion_data, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# only keep the top-k or in this case top 8 frames \n",
    "top_8_frames = emotion_sorted[:8]\n",
    "\n",
    "# setup to display the frames as a popup\n",
    "num_columns = 4  \n",
    "num_rows = (len(top_8_frames) // num_columns) + (len(top_8_frames) % num_columns > 0)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "gs = gridspec.GridSpec(num_rows, num_columns, figure=fig)\n",
    "\n",
    "# display in a grid format as a 2x4 \n",
    "for idx, (frame, emotion_text, confidence, frame_num) in enumerate(top_8_frames):\n",
    "    row = idx // num_columns\n",
    "    col = idx % num_columns\n",
    "\n",
    "    # conversions \n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    ax = fig.add_subplot(gs[row, col])\n",
    "    ax.imshow(frame_rgb)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # display the metadata at the bottom of the frame \n",
    "    ax.set_title(f\"Frame {frame_num}: {emotion_text}\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "popup = tk.Toplevel()\n",
    "popup.title(\"Emotion Detection\")\n",
    "\n",
    "# This portion was completed with ChatGPT to create a popup of the final display of the top-k results. \n",
    "# using Tkinter\n",
    "canvas = FigureCanvasTkAgg(fig, master=popup)  \n",
    "canvas.draw()\n",
    "canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "popup.mainloop()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
